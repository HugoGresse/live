(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[141],{8215:function(e,t,n){"use strict";var a=n(7294);t.Z=function(e){var t=e.children,n=e.hidden,o=e.className;return a.createElement("div",{role:"tabpanel",hidden:n,className:o},t)}},1395:function(e,t,n){"use strict";n.d(t,{Z:function(){return m}});var a=n(7294),o=n(944),s=n(6010),i="tabItem_1uMI",l="tabItemActive_2DSg";var r=37,c=39;var m=function(e){var t=e.lazy,n=e.block,m=e.defaultValue,d=e.values,p=e.groupId,u=e.className,f=(0,o.Z)(),h=f.tabGroupChoices,g=f.setTabGroupChoices,k=(0,a.useState)(m),v=k[0],b=k[1],y=a.Children.toArray(e.children),x=[];if(null!=p){var C=h[p];null!=C&&C!==v&&d.some((function(e){return e.value===C}))&&b(C)}var w=function(e){var t=e.currentTarget,n=x.indexOf(t),a=d[n].value;b(a),null!=p&&(g(p,a),setTimeout((function(){var e,n,a,o,s,i,r,c;(e=t.getBoundingClientRect(),n=e.top,a=e.left,o=e.bottom,s=e.right,i=window,r=i.innerHeight,c=i.innerWidth,n>=0&&s<=c&&o<=r&&a>=0)||(t.scrollIntoView({block:"center",behavior:"smooth"}),t.classList.add(l),setTimeout((function(){return t.classList.remove(l)}),2e3))}),150))},I=function(e){var t,n;switch(e.keyCode){case c:var a=x.indexOf(e.target)+1;n=x[a]||x[0];break;case r:var o=x.indexOf(e.target)-1;n=x[o]||x[x.length-1]}null==(t=n)||t.focus()};return a.createElement("div",{className:"tabs-container"},a.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.Z)("tabs",{"tabs--block":n},u)},d.map((function(e){var t=e.value,n=e.label;return a.createElement("li",{role:"tab",tabIndex:v===t?0:-1,"aria-selected":v===t,className:(0,s.Z)("tabs__item",i,{"tabs__item--active":v===t}),key:t,ref:function(e){return x.push(e)},onKeyDown:I,onFocus:w,onClick:w},n)}))),t?(0,a.cloneElement)(y.filter((function(e){return e.props.value===v}))[0],{className:"margin-vert--md"}):a.createElement("div",{className:"margin-vert--md"},y.map((function(e,t){return(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==v})}))))}},2924:function(e,t,n){"use strict";var a=n(7294).createContext(void 0);t.Z=a},9443:function(e,t,n){"use strict";var a=(0,n(7294).createContext)(void 0);t.Z=a},5350:function(e,t,n){"use strict";var a=n(7294),o=n(2924);t.Z=function(){var e=(0,a.useContext)(o.Z);if(null==e)throw new Error("`useThemeContext` is used outside of `Layout` Component. See https://docusaurus.io/docs/api/themes/configuration#usethemecontext.");return e}},944:function(e,t,n){"use strict";var a=n(7294),o=n(9443);t.Z=function(){var e=(0,a.useContext)(o.Z);if(null==e)throw new Error("`useUserPreferencesContext` is used outside of `Layout` Component.");return e}},9435:function(e,t,n){"use strict";n.d(t,{Z:function(){return s}});var a=n(7294),o="surveyLinkBox_YpLv";function s(e){var t="https://docs.google.com/forms/d/e/1FAIpQLScsB21xJWM_VANad5GcVkQqKB_BptS77axbunzs7ZkwoE5JUw/viewform?usp=pp_url&entry.1880917601="+e.docTitle.replace(/\s/g,"+");return a.createElement("a",{href:t,target:"_blank"},a.createElement("div",{className:o},"Share what we can improve!"))}},7630:function(e,t,n){"use strict";n.r(t),n.d(t,{default:function(){return y},frontMatter:function(){return g},metadata:function(){return k},toc:function(){return v}});var a=n(2122),o=n(9756),s=n(7294),i=n(3905),l=n(1395),r=n(8215);function c(e){var t=e.children;if(2!==t.length)throw new Error("ExampleDiffCodeTabs requires exactly two children: the diff and the code");return s.createElement(l.Z,{defaultValue:"diff",values:[{label:"Changes",value:"diff"},{label:"Entire File",value:"code"}]},s.createElement(r.Z,{value:"diff"},t[0]),s.createElement(r.Z,{value:"code"},t[1]))}var m=n(625),d=n.n(m);function p(){}p.V0CodeBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-tsx"}),"import * as React from 'react';\nimport {Text, View} from 'react-native';\nimport {useSafeAreaInsets} from 'react-native-safe-area-context';\n\nexport default function ImageClassificationDemo() {\n  // Get safe area insets to account for notches, etc.\n  const insets = useSafeAreaInsets();\n  return (\n    <View style={{marginTop: insets.top, marginBottom: insets.bottom}}>\n      <Text>Image Classification</Text>\n    </View>\n  );\n}\n")},p.V1CodeBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-tsx"}),"import * as React from 'react';\nimport {Text, StyleSheet, View} from 'react-native';\nimport {useSafeAreaInsets} from 'react-native-safe-area-context';\n\nexport default function ImageClassificationDemo() {\n  // Get safe area insets to account for notches, etc.\n  const insets = useSafeAreaInsets();\n  return (\n    <View\n      style={[\n        styles.container,\n        {marginTop: insets.top, marginBottom: insets.bottom},\n      ]}>\n      <Text style={styles.label}>Image Classification</Text>\n    </View>\n  );\n}\n\nconst styles = StyleSheet.create({\n  container: {\n    alignItems: 'center',\n    backgroundColor: '#ffffff',\n    display: 'flex',\n    flexGrow: 1,\n    padding: 20,\n  },\n  label: {\n    marginBottom: 10,\n  },\n});\n")},p.V1DiffBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-diff"}),"@@ -1,13 +1,30 @@\n import * as React from 'react';\n-import {Text, View} from 'react-native';\n+import {Text, StyleSheet, View} from 'react-native';\n import {useSafeAreaInsets} from 'react-native-safe-area-context';\n \n export default function ImageClassificationDemo() {\n   // Get safe area insets to account for notches, etc.\n   const insets = useSafeAreaInsets();\n   return (\n-    <View style={{marginTop: insets.top, marginBottom: insets.bottom}}>\n-      <Text>Image Classification</Text>\n+    <View\n+      style={[\n+        styles.container,\n+        {marginTop: insets.top, marginBottom: insets.bottom},\n+      ]}>\n+      <Text style={styles.label}>Image Classification</Text>\n     </View>\n   );\n }\n+\n+const styles = StyleSheet.create({\n+  container: {\n+    alignItems: 'center',\n+    backgroundColor: '#ffffff',\n+    display: 'flex',\n+    flexGrow: 1,\n+    padding: 20,\n+  },\n+  label: {\n+    marginBottom: 10,\n+  },\n+});\n")},p.V2CodeBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-tsx"}),"import * as React from 'react';\nimport {Text, StyleSheet, View} from 'react-native';\nimport {Camera} from 'react-native-pytorch-core';\nimport {useSafeAreaInsets} from 'react-native-safe-area-context';\n\nexport default function ImageClassificationDemo() {\n  // Get safe area insets to account for notches, etc.\n  const insets = useSafeAreaInsets();\n  return (\n    <View\n      style={[\n        styles.container,\n        {marginTop: insets.top, marginBottom: insets.bottom},\n      ]}>\n      <Text style={styles.label}>Image Classification</Text>\n      <Camera style={styles.camera} />\n    </View>\n  );\n}\n\nconst styles = StyleSheet.create({\n  container: {\n    alignItems: 'center',\n    backgroundColor: '#ffffff',\n    flexGrow: 1,\n    padding: 20,\n  },\n  label: {\n    marginBottom: 10,\n  },\n  camera: {\n    flexGrow: 1,\n    width: '100%',\n  },\n});\n")},p.V2DiffBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-diff"}),"@@ -1,5 +1,6 @@\n import * as React from 'react';\n import {Text, StyleSheet, View} from 'react-native';\n+import {Camera} from 'react-native-pytorch-core';\n import {useSafeAreaInsets} from 'react-native-safe-area-context';\n \n export default function ImageClassificationDemo() {\n@@ -12,6 +13,7 @@\n         {marginTop: insets.top, marginBottom: insets.bottom},\n       ]}>\n       <Text style={styles.label}>Image Classification</Text>\n+      <Camera style={styles.camera} />\n     </View>\n   );\n }\n@@ -20,11 +22,14 @@\n   container: {\n     alignItems: 'center',\n     backgroundColor: '#ffffff',\n-    display: 'flex',\n     flexGrow: 1,\n     padding: 20,\n   },\n   label: {\n     marginBottom: 10,\n   },\n+  camera: {\n+    flexGrow: 1,\n+    width: '100%',\n+  },\n });\n")},p.V3CodeBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-tsx"}),"import * as React from 'react';\nimport {Text, StyleSheet, View} from 'react-native';\nimport {Camera, Image} from 'react-native-pytorch-core';\nimport {useSafeAreaInsets} from 'react-native-safe-area-context';\n\nexport default function ImageClassificationDemo() {\n  // Get safe area insets to account for notches, etc.\n  const insets = useSafeAreaInsets();\n\n  async function handleImage(image: Image) {\n    // Log captured image to Metro console\n    console.log(image);\n    // It is important to release the image to avoid memory leaks\n    image.release();\n  }\n\n  return (\n    <View\n      style={[\n        styles.container,\n        {marginTop: insets.top, marginBottom: insets.bottom},\n      ]}>\n      <Text style={styles.label}>Image Classification</Text>\n      <Camera style={styles.camera} onCapture={handleImage} />\n    </View>\n  );\n}\n\nconst styles = StyleSheet.create({\n  container: {\n    alignItems: 'center',\n    backgroundColor: '#ffffff',\n    flexGrow: 1,\n    padding: 20,\n  },\n  label: {\n    marginBottom: 10,\n  },\n  camera: {\n    flexGrow: 1,\n    width: '100%',\n  },\n});\n")},p.V3DiffBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-diff"}),"@@ -1,11 +1,19 @@\n import * as React from 'react';\n import {Text, StyleSheet, View} from 'react-native';\n-import {Camera} from 'react-native-pytorch-core';\n+import {Camera, Image} from 'react-native-pytorch-core';\n import {useSafeAreaInsets} from 'react-native-safe-area-context';\n \n export default function ImageClassificationDemo() {\n   // Get safe area insets to account for notches, etc.\n   const insets = useSafeAreaInsets();\n+\n+  async function handleImage(image: Image) {\n+    // Log captured image to Metro console\n+    console.log(image);\n+    // It is important to release the image to avoid memory leaks\n+    image.release();\n+  }\n+\n   return (\n     <View\n       style={[\n@@ -13,7 +21,7 @@\n         {marginTop: insets.top, marginBottom: insets.bottom},\n       ]}>\n       <Text style={styles.label}>Image Classification</Text>\n-      <Camera style={styles.camera} />\n+      <Camera style={styles.camera} onCapture={handleImage} />\n     </View>\n   );\n }\n")},p.V4CodeBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-tsx"}),"import * as React from 'react';\nimport {Text, StyleSheet, View} from 'react-native';\nimport {Camera, Image, MobileModel} from 'react-native-pytorch-core';\nimport {useSafeAreaInsets} from 'react-native-safe-area-context';\n\nconst model = require('../../models/mobilenet_v3_small.ptl');\n\ntype ImageClassificationResult = {\n  maxIdx: number;\n  confidence: number;\n};\n\nexport default function ImageClassificationDemo() {\n  // Get safe area insets to account for notches, etc.\n  const insets = useSafeAreaInsets();\n\n  async function handleImage(image: Image) {\n    const inferenceResult =\n      await MobileModel.execute<ImageClassificationResult>(model, {\n        image,\n      });\n\n    // Log model inference result to Metro console\n    console.log(inferenceResult);\n\n    // It is important to release the image to avoid memory leaks\n    image.release();\n  }\n\n  return (\n    <View\n      style={[\n        styles.container,\n        {marginTop: insets.top, marginBottom: insets.bottom},\n      ]}>\n      <Text style={styles.label}>Image Classification</Text>\n      <Camera style={styles.camera} onCapture={handleImage} />\n    </View>\n  );\n}\n\nconst styles = StyleSheet.create({\n  container: {\n    alignItems: 'center',\n    backgroundColor: '#ffffff',\n    flexGrow: 1,\n    padding: 20,\n  },\n  label: {\n    marginBottom: 10,\n  },\n  camera: {\n    flexGrow: 1,\n    width: '100%',\n  },\n});\n")},p.V4DiffBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-diff"}),"@@ -1,15 +1,28 @@\n import * as React from 'react';\n import {Text, StyleSheet, View} from 'react-native';\n-import {Camera, Image} from 'react-native-pytorch-core';\n+import {Camera, Image, MobileModel} from 'react-native-pytorch-core';\n import {useSafeAreaInsets} from 'react-native-safe-area-context';\n \n+const model = require('../../models/mobilenet_v3_small.ptl');\n+\n+type ImageClassificationResult = {\n+  maxIdx: number;\n+  confidence: number;\n+};\n+\n export default function ImageClassificationDemo() {\n   // Get safe area insets to account for notches, etc.\n   const insets = useSafeAreaInsets();\n \n   async function handleImage(image: Image) {\n-    // Log captured image to Metro console\n-    console.log(image);\n+    const inferenceResult =\n+      await MobileModel.execute<ImageClassificationResult>(model, {\n+        image,\n+      });\n+\n+    // Log model inference result to Metro console\n+    console.log(inferenceResult);\n+\n     // It is important to release the image to avoid memory leaks\n     image.release();\n   }\n")},p.V5CodeBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-tsx"}),"import * as React from 'react';\nimport {Text, StyleSheet, View} from 'react-native';\nimport {Camera, Image, MobileModel} from 'react-native-pytorch-core';\nimport {useSafeAreaInsets} from 'react-native-safe-area-context';\n\nconst model = require('../../models/mobilenet_v3_small.ptl');\n\ntype ImageClassificationResult = {\n  maxIdx: number;\n  confidence: number;\n};\n\nconst ImageClasses = require('../MobileNetV3Classes');\n\nexport default function ImageClassificationDemo() {\n  // Get safe area insets to account for notches, etc.\n  const insets = useSafeAreaInsets();\n\n  async function handleImage(image: Image) {\n    const {result} = await MobileModel.execute<ImageClassificationResult>(\n      model,\n      {\n        image,\n      },\n    );\n\n    // Get max index (argmax) result to resolve the top class name\n    const topClass = ImageClasses[result.maxIdx];\n\n    // Log top class to Metro console\n    console.log(topClass);\n\n    // It is important to release the image to avoid memory leaks\n    image.release();\n  }\n\n  return (\n    <View\n      style={[\n        styles.container,\n        {marginTop: insets.top, marginBottom: insets.bottom},\n      ]}>\n      <Text style={styles.label}>Image Classification</Text>\n      <Camera style={styles.camera} onCapture={handleImage} />\n    </View>\n  );\n}\n\nconst styles = StyleSheet.create({\n  container: {\n    alignItems: 'center',\n    backgroundColor: '#ffffff',\n    flexGrow: 1,\n    padding: 20,\n  },\n  label: {\n    marginBottom: 10,\n  },\n  camera: {\n    flexGrow: 1,\n    width: '100%',\n  },\n});\n")},p.V5DiffBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-diff"}),"@@ -10,18 +10,25 @@\n   confidence: number;\n };\n \n+const ImageClasses = require('../MobileNetV3Classes');\n+\n export default function ImageClassificationDemo() {\n   // Get safe area insets to account for notches, etc.\n   const insets = useSafeAreaInsets();\n \n   async function handleImage(image: Image) {\n-    const inferenceResult =\n-      await MobileModel.execute<ImageClassificationResult>(model, {\n+    const {result} = await MobileModel.execute<ImageClassificationResult>(\n+      model,\n+      {\n         image,\n-      });\n+      },\n+    );\n+\n+    // Get max index (argmax) result to resolve the top class name\n+    const topClass = ImageClasses[result.maxIdx];\n \n-    // Log model inference result to Metro console\n-    console.log(inferenceResult);\n+    // Log top class to Metro console\n+    console.log(topClass);\n \n     // It is important to release the image to avoid memory leaks\n     image.release();\n")},p.V6CodeBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-tsx"}),"import * as React from 'react';\nimport {Text, StyleSheet, View} from 'react-native';\nimport {Camera, Image, MobileModel} from 'react-native-pytorch-core';\nimport {useSafeAreaInsets} from 'react-native-safe-area-context';\n\nconst model = require('../../models/mobilenet_v3_small.ptl');\n\ntype ImageClassificationResult = {\n  maxIdx: number;\n  confidence: number;\n};\n\nconst ImageClasses = require('../MobileNetV3Classes');\n\nexport default function ImageClassificationDemo() {\n  // Get safe area insets to account for notches, etc.\n  const insets = useSafeAreaInsets();\n\n  // Component state that holds the detected object class\n  const [objectClass, setObjectClass] = React.useState<string>('');\n\n  async function handleImage(image: Image) {\n    const {result} = await MobileModel.execute<ImageClassificationResult>(\n      model,\n      {\n        image,\n      },\n    );\n\n    // Get max index (argmax) result to resolve the top class name\n    const topClass = ImageClasses[result.maxIdx];\n\n    // Set object class state to be the top class detected in the image\n    setObjectClass(topClass);\n\n    // It is important to release the image to avoid memory leaks\n    image.release();\n  }\n\n  return (\n    <View\n      style={[\n        styles.container,\n        {marginTop: insets.top, marginBottom: insets.bottom},\n      ]}>\n      <Text style={styles.label}>Object: {objectClass}</Text>\n      <Camera style={styles.camera} onCapture={handleImage} />\n    </View>\n  );\n}\n\nconst styles = StyleSheet.create({\n  container: {\n    alignItems: 'center',\n    backgroundColor: '#ffffff',\n    flexGrow: 1,\n    padding: 20,\n  },\n  label: {\n    marginBottom: 10,\n  },\n  camera: {\n    flexGrow: 1,\n    width: '100%',\n  },\n});\n")},p.V6DiffBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-diff"}),"@@ -16,6 +16,9 @@\n   // Get safe area insets to account for notches, etc.\n   const insets = useSafeAreaInsets();\n \n+  // Component state that holds the detected object class\n+  const [objectClass, setObjectClass] = React.useState<string>('');\n+\n   async function handleImage(image: Image) {\n     const {result} = await MobileModel.execute<ImageClassificationResult>(\n       model,\n@@ -27,8 +30,8 @@\n     // Get max index (argmax) result to resolve the top class name\n     const topClass = ImageClasses[result.maxIdx];\n \n-    // Log top class to Metro console\n-    console.log(topClass);\n+    // Set object class state to be the top class detected in the image\n+    setObjectClass(topClass);\n \n     // It is important to release the image to avoid memory leaks\n     image.release();\n@@ -40,7 +43,7 @@\n         styles.container,\n         {marginTop: insets.top, marginBottom: insets.bottom},\n       ]}>\n-      <Text style={styles.label}>Image Classification</Text>\n+      <Text style={styles.label}>Object: {objectClass}</Text>\n       <Camera style={styles.camera} onCapture={handleImage} />\n     </View>\n   );\n")},p.V7CodeBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-tsx"}),"import * as React from 'react';\nimport {Text, StyleSheet, View} from 'react-native';\nimport {Camera, Image, MobileModel} from 'react-native-pytorch-core';\nimport {useSafeAreaInsets} from 'react-native-safe-area-context';\n\nconst model = require('../../models/mobilenet_v3_small.ptl');\n\ntype ImageClassificationResult = {\n  maxIdx: number;\n  confidence: number;\n};\n\nconst ImageClasses = require('../MobileNetV3Classes');\n\nexport default function ImageClassificationDemo() {\n  // Get safe area insets to account for notches, etc.\n  const insets = useSafeAreaInsets();\n\n  // Component state that holds the detected object class\n  const [objectClass, setObjectClass] = React.useState<string>('');\n\n  async function handleImage(image: Image) {\n    const {result} = await MobileModel.execute<ImageClassificationResult>(\n      model,\n      {\n        image,\n      },\n    );\n\n    if (result.confidence > 0.3) {\n      // Get max index (argmax) result to resolve the top class name\n      const topClass = ImageClasses[result.maxIdx];\n\n      // Set object class state to be the top class detected in the image\n      setObjectClass(topClass);\n    } else {\n      // Reset the object class if confidence value is low\n      setObjectClass('');\n    }\n\n    // It is important to release the image to avoid memory leaks\n    image.release();\n  }\n\n  return (\n    <View\n      style={[\n        styles.container,\n        {marginTop: insets.top, marginBottom: insets.bottom},\n      ]}>\n      <Text style={styles.label}>Object: {objectClass}</Text>\n      <Camera style={styles.camera} onCapture={handleImage} />\n    </View>\n  );\n}\n\nconst styles = StyleSheet.create({\n  container: {\n    alignItems: 'center',\n    backgroundColor: '#ffffff',\n    flexGrow: 1,\n    padding: 20,\n  },\n  label: {\n    marginBottom: 10,\n  },\n  camera: {\n    flexGrow: 1,\n    width: '100%',\n  },\n});\n")},p.V7DiffBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-diff"}),"@@ -27,11 +27,16 @@\n       },\n     );\n \n-    // Get max index (argmax) result to resolve the top class name\n-    const topClass = ImageClasses[result.maxIdx];\n+    if (result.confidence > 0.3) {\n+      // Get max index (argmax) result to resolve the top class name\n+      const topClass = ImageClasses[result.maxIdx];\n \n-    // Set object class state to be the top class detected in the image\n-    setObjectClass(topClass);\n+      // Set object class state to be the top class detected in the image\n+      setObjectClass(topClass);\n+    } else {\n+      // Reset the object class if confidence value is low\n+      setObjectClass('');\n+    }\n \n     // It is important to release the image to avoid memory leaks\n     image.release();\n")},p.V8CodeBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-tsx"}),"import * as React from 'react';\nimport {Text, StyleSheet, View} from 'react-native';\nimport {Camera, Image, MobileModel} from 'react-native-pytorch-core';\nimport {useSafeAreaInsets} from 'react-native-safe-area-context';\n\nconst model = require('../../models/mobilenet_v3_small.ptl');\n\ntype ImageClassificationResult = {\n  maxIdx: number;\n  confidence: number;\n};\n\nconst ImageClasses = require('../MobileNetV3Classes');\n\nexport default function ImageClassificationDemo() {\n  // Get safe area insets to account for notches, etc.\n  const insets = useSafeAreaInsets();\n\n  // Component state that holds the detected object class\n  const [objectClass, setObjectClass] = React.useState<string>('');\n\n  async function handleImage(image: Image) {\n    const {result} = await MobileModel.execute<ImageClassificationResult>(\n      model,\n      {\n        image,\n      },\n    );\n\n    if (result.confidence > 0.3) {\n      // Get max index (argmax) result to resolve the top class name\n      const topClass = ImageClasses[result.maxIdx];\n\n      // Set object class state to be the top class detected in the image\n      setObjectClass(topClass);\n    } else {\n      // Reset the object class if confidence value is low\n      setObjectClass('');\n    }\n\n    // It is important to release the image to avoid memory leaks\n    image.release();\n  }\n\n  return (\n    <View\n      style={[\n        styles.container,\n        {marginTop: insets.top, marginBottom: insets.bottom},\n      ]}>\n      <Text style={styles.label}>Object: {objectClass}</Text>\n      <Camera\n        style={styles.camera}\n        onFrame={handleImage}\n        hideCaptureButton={true}\n      />\n    </View>\n  );\n}\n\nconst styles = StyleSheet.create({\n  container: {\n    alignItems: 'center',\n    backgroundColor: '#ffffff',\n    flexGrow: 1,\n    padding: 20,\n  },\n  label: {\n    marginBottom: 10,\n  },\n  camera: {\n    flexGrow: 1,\n    width: '100%',\n  },\n});\n")},p.V8DiffBlock=function(e){return s.createElement(d(),(0,a.Z)({},e,{className:"language-diff"}),"@@ -49,7 +49,11 @@\n         {marginTop: insets.top, marginBottom: insets.bottom},\n       ]}>\n       <Text style={styles.label}>Object: {objectClass}</Text>\n-      <Camera style={styles.camera} onCapture={handleImage} />\n+      <Camera\n+        style={styles.camera}\n+        onFrame={handleImage}\n+        hideCaptureButton={true}\n+      />\n     </View>\n   );\n }\n")};var u=p,f=n(9435),h=["components"],g={id:"image-classification",sidebar_position:2},k={unversionedId:"tutorials/image-classification",id:"tutorials/image-classification",isDocsHomePage:!1,title:"Image Classification",description:"In this tutorial, you will build an app that can take pictures and classify objects in each image using an on-device image classification model.",source:"@site/docs/tutorials/image-classification.mdx",sourceDirName:"tutorials",slug:"/tutorials/image-classification",permalink:"/docs/tutorials/image-classification",editUrl:"https://github.com/pytorch/live/edit/main/website/docs/tutorials/image-classification.mdx",version:"current",sidebarPosition:2,frontMatter:{id:"image-classification",sidebar_position:2},sidebar:"docs",previous:{title:"Add Package To Existing App",permalink:"/docs/tutorials/add-package"},next:{title:"Question Answering",permalink:"/docs/tutorials/question-answering"}},v=[{value:"In this tutorial, you will build an app that can take pictures and classify objects in each image using an on-device image classification model.",id:"in-this-tutorial-you-will-build-an-app-that-can-take-pictures-and-classify-objects-in-each-image-using-an-on-device-image-classification-model",children:[]},{value:"Initialize New Project",id:"initialize-new-project",children:[{value:"Run the project in the Android emulator or iOS Simulator",id:"run-the-project-in-the-android-emulator-or-ios-simulator",children:[]}]},{value:"Image Classification Demo",id:"image-classification-demo",children:[{value:"Style the component",id:"style-the-component",children:[]},{value:"Add camera component",id:"add-camera-component",children:[]},{value:"Add capture callback to camera",id:"add-capture-callback-to-camera",children:[]},{value:"Run model inference",id:"run-model-inference",children:[]},{value:"Get top image class",id:"get-top-image-class",children:[]},{value:"Show top image class",id:"show-top-image-class",children:[]},{value:"Confidence threshold",id:"confidence-threshold",children:[]},{value:"Frame-by-Frame image processing",id:"frame-by-frame-image-processing",children:[]}]},{value:"Next steps",id:"next-steps",children:[{value:"Challenge",id:"challenge",children:[]}]},{value:"Use custom image classification model",id:"use-custom-image-classification-model",children:[]},{value:"Give us feedback",id:"give-us-feedback",children:[]}],b={toc:v};function y(e){var t=e.components,s=(0,o.Z)(e,h);return(0,i.kt)("wrapper",(0,a.Z)({},b,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("div",{className:"tutorial-page"},(0,i.kt)("h3",{id:"in-this-tutorial-you-will-build-an-app-that-can-take-pictures-and-classify-objects-in-each-image-using-an-on-device-image-classification-model"},"In this tutorial, you will build an app that can take pictures and classify objects in each image using an on-device image classification model."),(0,i.kt)("p",null,"If you haven't installed the PyTorch Live CLI yet, please ",(0,i.kt)("a",{parentName:"p",href:"get-started"},"follow this tutorial")," to get started."),(0,i.kt)("h2",{id:"initialize-new-project"},"Initialize New Project"),(0,i.kt)("p",null,"Let's start by initializing a new project ",(0,i.kt)("inlineCode",{parentName:"p"},"ImageClassificationTutorial")," with the PyTorch Live CLI."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"npx torchlive-cli init ImageClassificationTutorial\n")),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"The project init can take a few minutes depending on your internet connection and your computer."))),(0,i.kt)("p",null,"After completion, navigate to the ",(0,i.kt)("inlineCode",{parentName:"p"},"ImageClassificationTutorial")," directory created by the ",(0,i.kt)("inlineCode",{parentName:"p"},"init")," command."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"cd ImageClassificationTutorial\n")),(0,i.kt)("h3",{id:"run-the-project-in-the-android-emulator-or-ios-simulator"},"Run the project in the Android emulator or iOS Simulator"),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"run-android")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"run-ios")," commands from the PyTorch Live CLI allow you to run the image classification project in the Android emulator or iOS Simulator."),(0,i.kt)(l.Z,{defaultValue:"android",values:[{label:"Android",value:"android"},{label:"iOS (Simulator)",value:"ios-sim"}],mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"android",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"npx torchlive-cli run-android\n")),(0,i.kt)("p",null,"  The app will deploy and run on your physical Android device if it is connected to your computer via USB, and it is in developer mode. There are more details on that in the ",(0,i.kt)("a",{parentName:"p",href:"get-started"},"Get Started tutorial"),"."),(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(1701).Z,title:"Screenshot of app after fresh init with CLI"}))),(0,i.kt)(r.Z,{value:"ios-sim",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"npx torchlive-cli run-ios\n")),(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(7600).Z,title:"Screenshot of app after fresh init with CLI"})))),(0,i.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"tip")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Keep the app open and running! Any code change will immediately be reflected after saving."))),(0,i.kt)("h2",{id:"image-classification-demo"},"Image Classification Demo"),(0,i.kt)("p",null,"Let's get started with the UI for the image classification. Go ahead and start by copying the following code into the file ",(0,i.kt)("inlineCode",{parentName:"p"},"src/demos/MyDemos.tsx"),":"),(0,i.kt)(u.V0CodeBlock,{title:"src/demos/MyDemos.tsx"}),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"The ",(0,i.kt)("inlineCode",{parentName:"p"},"MyDemos.tsx")," already contains code. Replace the code with the code below."))),(0,i.kt)(l.Z,{defaultValue:"android",values:[{label:"Android",value:"android"},{label:"iOS (Simulator)",value:"ios-sim"}],mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"android",mdxType:"TabItem"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(2981).Z,title:"Screenshot of initial code with text component"}))),(0,i.kt)(r.Z,{value:"ios-sim",mdxType:"TabItem"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(7815).Z,title:"Screenshot of initial code with text component"})))),(0,i.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"tip")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},'The app starts with the "Examples" tab open. In order to see the changes you just made to the ',(0,i.kt)("inlineCode",{parentName:"p"},"MyDemos.tsx"),', tap on the "My Demos" tab bar item at the bottom of the screen.'))),(0,i.kt)("h3",{id:"style-the-component"},"Style the component"),(0,i.kt)("p",null,"Great! Let's add some basic styling to the app UI. The styles will change the ",(0,i.kt)("inlineCode",{parentName:"p"},"View")," component background to ",(0,i.kt)("inlineCode",{parentName:"p"},"#ffffff"),", spans container view to maximum available width and height, centers components horizontally, and adds a padding of ",(0,i.kt)("inlineCode",{parentName:"p"},"20")," pixels. The ",(0,i.kt)("inlineCode",{parentName:"p"},"Text")," component will have a margin at the bottom to provide spacing between the text label and the ",(0,i.kt)("inlineCode",{parentName:"p"},"Camera")," component that will be added in the next steps."),(0,i.kt)(c,{mdxType:"ExampleDiffCodeTabs"},(0,i.kt)(u.V1DiffBlock,null),(0,i.kt)(u.V1CodeBlock,{title:"src/demos/MyDemos.tsx"})),(0,i.kt)(l.Z,{defaultValue:"android",values:[{label:"Android",value:"android"},{label:"iOS (Simulator)",value:"ios-sim"}],mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"android",mdxType:"TabItem"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(501).Z,title:"Screenshot after applying simple component styles"}))),(0,i.kt)(r.Z,{value:"ios-sim",mdxType:"TabItem"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(8393).Z,title:"Screenshot after applying simple component styles"})))),(0,i.kt)("h3",{id:"add-camera-component"},"Add camera component"),(0,i.kt)("p",null,"Next, let's add a ",(0,i.kt)("inlineCode",{parentName:"p"},"Camera")," component to take pictures that can be used later for the ML model inference to classify what object is in the picture. The camera will also get a basic style to fill the remaining space in the container."),(0,i.kt)(c,{mdxType:"ExampleDiffCodeTabs"},(0,i.kt)(u.V2DiffBlock,null),(0,i.kt)(u.V2CodeBlock,{title:"src/demos/MyDemos.tsx"})),(0,i.kt)(l.Z,{defaultValue:"android",values:[{label:"Android",value:"android"},{label:"iOS (Simulator)",value:"ios-sim"},{label:"iOS",value:"ios"}],mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"android",mdxType:"TabItem"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(5955).Z,title:"Screenshot with camera component"}))),(0,i.kt)(r.Z,{value:"ios-sim",mdxType:"TabItem"},(0,i.kt)("p",null,"  In order to get the camera to work on iOS, you'll have to run the app on a physical iOS device. For more details, please check out the ",(0,i.kt)("a",{parentName:"p",href:"https://reactnative.dev/docs/running-on-device"},"Running On Device docs on the React Native website"),"."),(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(8793).Z,title:"Screenshot showing camera not available on iOS Simulator"}))),(0,i.kt)(r.Z,{value:"ios",mdxType:"TabItem"},(0,i.kt)("p",null,"  In order to get the camera to work on iOS, you'll have to run the app on a physical iOS device. For more details, please check out the ",(0,i.kt)("a",{parentName:"p",href:"https://reactnative.dev/docs/running-on-device"},"Running On Device docs on the React Native website"),"."),(0,i.kt)("p",null,"  Open the ImageClassificationTutorial Xcode workspace to run the project on a physical iOS device."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"open ios/ImageClassificationTutorial.xcworkspace\n")),(0,i.kt)("p",null,"  Run app on physical device."),(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(6063).Z,title:"Screenshot with camera component"})))),(0,i.kt)("h3",{id:"add-capture-callback-to-camera"},"Add capture callback to camera"),(0,i.kt)("p",null,"To receive an image whenever the camera capture button is pressed, we add an async ",(0,i.kt)("inlineCode",{parentName:"p"},"handleImage")," function and set it for the ",(0,i.kt)("inlineCode",{parentName:"p"},"onCapture")," property of the ",(0,i.kt)("inlineCode",{parentName:"p"},"Camera")," component. This ",(0,i.kt)("inlineCode",{parentName:"p"},"handleImage")," function will be called with an image from the camera when the capture button is pressed."),(0,i.kt)("p",null,"As a first step, let's log image to the console."),(0,i.kt)("div",{className:"admonition admonition-caution alert alert--warning"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"The ",(0,i.kt)("inlineCode",{parentName:"p"},"image.release()")," function call is important to release the memory allocated for the image object. This is a vital step to make sure we don't run out of memory on images we no longer need."))),(0,i.kt)(c,{mdxType:"ExampleDiffCodeTabs"},(0,i.kt)(u.V3DiffBlock,null),(0,i.kt)(u.V3CodeBlock,{title:"src/demos/MyDemos.tsx"})),(0,i.kt)("p",null,"Click on camera capture button and check logged output in terminal. It will log a JavaScript representation of the image to the console every time you click the capture button."),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(9988).Z})),(0,i.kt)("h3",{id:"run-model-inference"},"Run model inference"),(0,i.kt)("p",null,"Fantastic! Now let's use the image and run inference on a captured image."),(0,i.kt)("p",null,"We'll require the MobileNet V3 (small) model and add the ",(0,i.kt)("inlineCode",{parentName:"p"},"ImageClassificationResult")," type for type-safety. Then, we call the ",(0,i.kt)("inlineCode",{parentName:"p"},"execute")," function on the ",(0,i.kt)("inlineCode",{parentName:"p"},"MobileModel")," object with the model as first argument and an object with the image as second argument."),(0,i.kt)("p",null,"Don't forget the ",(0,i.kt)("inlineCode",{parentName:"p"},"await")," keyword for the ",(0,i.kt)("inlineCode",{parentName:"p"},"MobileModel.execute")," function call!"),(0,i.kt)("p",null,"Last, let's log the inference result to the console."),(0,i.kt)(c,{mdxType:"ExampleDiffCodeTabs"},(0,i.kt)(u.V4DiffBlock,null),(0,i.kt)(u.V4CodeBlock,{title:"src/demos/MyDemos.tsx"})),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(6673).Z})),(0,i.kt)("p",null,"The logged inference result is a JavaScript object containing the inference result including the ",(0,i.kt)("inlineCode",{parentName:"p"},"maxIdx")," (argmax result) mapping to the top class detected in the image, a confidence value for this class to be correct, and inference metrics (i.e., inference time, pack time, unpack time, and total time)."),(0,i.kt)("h3",{id:"get-top-image-class"},"Get top image class"),(0,i.kt)("p",null,"Ok! So, we have an ",(0,i.kt)("inlineCode",{parentName:"p"},"maxIdx")," number as inference result (i.e., ",(0,i.kt)("inlineCode",{parentName:"p"},"673"),"). It's not sensible to show a ",(0,i.kt)("inlineCode",{parentName:"p"},"maxIdx")," to the user, so let's get label for the top class. For this, we need to import the image classes for this model, which is the ",(0,i.kt)("inlineCode",{parentName:"p"},"MobileNetV3Classes")," JSON file containing an array of 1000 class labels. The ",(0,i.kt)("inlineCode",{parentName:"p"},"maxIdx")," maps to a label representing the top class."),(0,i.kt)("p",null,"Here, we require the JSON file into the ",(0,i.kt)("inlineCode",{parentName:"p"},"ImageClasses")," variable and use ",(0,i.kt)("inlineCode",{parentName:"p"},"ImageClasses")," to retrieve the label for the top class using the ",(0,i.kt)("inlineCode",{parentName:"p"},"maxIdx")," returned from the inference."),(0,i.kt)("p",null,"Let's see what the ",(0,i.kt)("inlineCode",{parentName:"p"},"maxIdx")," ",(0,i.kt)("inlineCode",{parentName:"p"},"673")," resolves into by logging the ",(0,i.kt)("inlineCode",{parentName:"p"},"topClass")," label to the console!"),(0,i.kt)(c,{mdxType:"ExampleDiffCodeTabs"},(0,i.kt)(u.V5DiffBlock,null),(0,i.kt)(u.V5CodeBlock,{title:"src/demos/MyDemos.tsx"})),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(7299).Z})),(0,i.kt)("p",null,"It looks like the model classified the image as ",(0,i.kt)("inlineCode",{parentName:"p"},"mouse, computer mouse"),". The next section will reveal if this is correct!"),(0,i.kt)("h3",{id:"show-top-image-class"},"Show top image class"),(0,i.kt)("p",null,"Instead of having the end-user looking at a console log, we will render the top image class in the app. We'll add a state for the ",(0,i.kt)("inlineCode",{parentName:"p"},"objectClass")," using a React Hook, and when a class is detected, we'll set the top class as object class using the ",(0,i.kt)("inlineCode",{parentName:"p"},"setObjectClass")," function."),(0,i.kt)("p",null,"The user interface will automatically re-render whenever the ",(0,i.kt)("inlineCode",{parentName:"p"},"setObjectClass")," function is called with a new value, so you don't have to worry about calling anything else besides this function. On re-render, the ",(0,i.kt)("inlineCode",{parentName:"p"},"objectClass")," variable will have this new value, so we can use it to render it on the screen."),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"The ",(0,i.kt)("inlineCode",{parentName:"p"},"React.useState")," is a React Hook. Hooks allow React function components, like our ",(0,i.kt)("inlineCode",{parentName:"p"},"ImageClassificationTutorial")," function component, to remember things."),(0,i.kt)("p",{parentName:"div"},"For more information on ",(0,i.kt)("a",{parentName:"p",href:"https://reactjs.org/docs/hooks-intro.html"},"React Hooks"),", head over to the React docs where you can read or watch explanations."))),(0,i.kt)(c,{mdxType:"ExampleDiffCodeTabs"},(0,i.kt)(u.V6DiffBlock,null),(0,i.kt)(u.V6CodeBlock,{title:"src/demos/MyDemos.tsx"})),(0,i.kt)(l.Z,{defaultValue:"android",values:[{label:"Android",value:"android"},{label:"iOS",value:"ios"}],mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"android",mdxType:"TabItem"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(5680).Z,title:"Screenshot for showing the label of the captured object"}))),(0,i.kt)(r.Z,{value:"ios",mdxType:"TabItem"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(2893).Z,title:"Screenshot for showing the label of the captured object"})))),(0,i.kt)("p",null,"It looks like the model correctly classified the object in the image as a ",(0,i.kt)("inlineCode",{parentName:"p"},"mouse, computer mouse"),"!"),(0,i.kt)("h3",{id:"confidence-threshold"},"Confidence threshold"),(0,i.kt)("p",null,"Nice! The model will return a top class for what it thinks is in the image. However, it's not always 100% confident about each classification, and therefore returns a ",(0,i.kt)("inlineCode",{parentName:"p"},"confidence")," value as part of the result. To see what the ",(0,i.kt)("inlineCode",{parentName:"p"},"metrics")," looks like, have a look at the step where we logged the ",(0,i.kt)("inlineCode",{parentName:"p"},"inferenceResult")," to the console!"),(0,i.kt)("p",null,"Let's use this confidence value as a threshold, and only show top classes where the model has a confidence higher than ",(0,i.kt)("inlineCode",{parentName:"p"},"0.3")," (the confidence range is ","[0, 1]",")."),(0,i.kt)(c,{mdxType:"ExampleDiffCodeTabs"},(0,i.kt)(u.V7DiffBlock,null),(0,i.kt)(u.V7CodeBlock,{title:"src/demos/MyDemos.tsx"})),(0,i.kt)("h3",{id:"frame-by-frame-image-processing"},"Frame-by-Frame image processing"),(0,i.kt)("p",null,"As a bonus, you can change the ",(0,i.kt)("inlineCode",{parentName:"p"},"onCapture")," property to the ",(0,i.kt)("inlineCode",{parentName:"p"},"onFrame")," property to do a frame-by-frame image classification, so you don't have to repeatedly press the capture button, and you can roam the phone around your place to see what the model can detect correctly."),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Known problem: If the images aren't immediately processed frame by frame, flip the camera twice."))),(0,i.kt)(c,{mdxType:"ExampleDiffCodeTabs"},(0,i.kt)(u.V8DiffBlock,null),(0,i.kt)(u.V8CodeBlock,{title:"src/demos/MyDemos.tsx"})),(0,i.kt)(l.Z,{defaultValue:"android",values:[{label:"Android",value:"android"},{label:"iOS",value:"ios"}],mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"android",mdxType:"TabItem"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(8419).Z,title:"Screenshot for showing the label of the captured object"}))),(0,i.kt)(r.Z,{value:"ios",mdxType:"TabItem"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{src:n(840).Z,title:"Screenshot for showing the label of the captured object"})))),(0,i.kt)("p",null,"Congratulations! You finished your first PyTorch Live tutorial."),(0,i.kt)("h2",{id:"next-steps"},"Next steps"),(0,i.kt)("p",null,"PyTorch Live comes with three image classification models that are ready to use. In the example code provided in this tutorial, we use ",(0,i.kt)("inlineCode",{parentName:"p"},"mobilenet_v3_small.ptl")," for inference, but feel free to try out the others by replacing the ",(0,i.kt)("inlineCode",{parentName:"p"},"model")," with code from the tabbed viewer below."),(0,i.kt)(l.Z,{defaultValue:"mobilenet-v3-small",values:[{label:"MobileNet v3 Small",value:"mobilenet-v3-small"},{label:"MobileNet v3 Large",value:"mobilenet-v3-large"},{label:"ResNet-18",value:"resnet18"}],mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"mobilenet-v3-small",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-tsx"},"const model = require('../../models/mobilenet_v3_small.ptl');\n"))),(0,i.kt)(r.Z,{value:"mobilenet-v3-large",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-tsx"},"const model = require('../../models/mobilenet_v3_large.ptl');\n"))),(0,i.kt)(r.Z,{value:"resnet18",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-tsx"},"const model = require('../../models/resnet18.ptl');\n")))),(0,i.kt)("h3",{id:"challenge"},"Challenge"),(0,i.kt)("p",null,"Rank the models from slowest to fastest!"),(0,i.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"tip")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Log the ",(0,i.kt)("inlineCode",{parentName:"p"},"metrics")," from the inference result to the console or render it on the screen!"))),(0,i.kt)("h2",{id:"use-custom-image-classification-model"},"Use custom image classification model"),(0,i.kt)("p",null,"You can follow the ",(0,i.kt)("a",{parentName:"p",href:"prepare-custom-model"},"Prepare Custom Model")," tutorial to prepare your own classification model that you can plug into the demo code provided here."),(0,i.kt)("h2",{id:"give-us-feedback"},"Give us feedback"),(0,i.kt)(f.Z,{docTitle:"Image Classification",mdxType:"SurveyLinkButton"})))}y.isMDXComponent=!0},5955:function(e,t,n){"use strict";t.Z=n.p+"assets/images/camera-cfa5daf858e9f57c00cda73d72a830bd.png"},5680:function(e,t,n){"use strict";t.Z=n.p+"assets/images/captured-object-60289163c7ece05e488019709ddc72cc.png"},1701:function(e,t,n){"use strict";t.Z=n.p+"assets/images/first-run-25e23131120ec484ac24093ae32f4e52.png"},8419:function(e,t,n){"use strict";t.Z=n.p+"assets/images/frame-by-frame-460c5827008a513a5b34117ad7e03ca7.gif"},2981:function(e,t,n){"use strict";t.Z=n.p+"assets/images/image-classification-text-0536b337199259fb1f1c5f8549bd4470.png"},501:function(e,t,n){"use strict";t.Z=n.p+"assets/images/simple-styles-d74a4018405696a7b6f20f89babbcfc1.png"},8793:function(e,t,n){"use strict";t.Z=n.p+"assets/images/camera-sim-396ca3119d1888b7ad06b40e3da2921c.png"},6063:function(e,t,n){"use strict";t.Z=n.p+"assets/images/camera-99cf8ccc3947d11a33daaf24a28deec9.png"},2893:function(e,t,n){"use strict";t.Z=n.p+"assets/images/captured-object-541f7b1c7b1326cc53dfd2afe0d41d41.png"},7600:function(e,t,n){"use strict";t.Z=n.p+"assets/images/first-run-141ff37378121bc763a8a597ec83ab89.png"},840:function(e,t,n){"use strict";t.Z=n.p+"assets/images/frame-by-frame-c66306dcceb441f7dd5c7ce4a85c8a19.gif"},7815:function(e,t,n){"use strict";t.Z=n.p+"assets/images/image-classification-text-3f5f36b31b32ef03d1d7ba7668e5d846.png"},8393:function(e,t,n){"use strict";t.Z=n.p+"assets/images/simple-styles-f4a793ac40f078094d19aa5ddeb4fde8.png"},6673:function(e,t,n){"use strict";t.Z=n.p+"assets/images/metro-console-log-inference-result-e730bf3f2d3b2c44b15ad4b584a3b27b.png"},9988:function(e,t,n){"use strict";t.Z=n.p+"assets/images/metro-console-log-3e59e11ba97fa6e320c3c61cfec51429.png"},7299:function(e,t,n){"use strict";t.Z=n.p+"assets/images/metro-resolved-top-class-e0e020262469f4581119ce8dce95d900.png"}}]);