(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1469],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return p},kt:function(){return m}});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=r.createContext({}),c=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},p=function(e){var t=c(e.components);return r.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(n),m=i,h=u["".concat(s,".").concat(m)]||u[m]||d[m]||o;return n?r.createElement(h,a(a({ref:t},p),{},{components:n})):r.createElement(h,a({ref:t},p))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,a=new Array(o);a[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,a[1]=l;for(var c=2;c<o;c++)a[c]=n[c];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},8215:function(e,t,n){"use strict";var r=n(7294);t.Z=function(e){var t=e.children,n=e.hidden,i=e.className;return r.createElement("div",{role:"tabpanel",hidden:n,className:i},t)}},1395:function(e,t,n){"use strict";n.d(t,{Z:function(){return p}});var r=n(7294),i=n(944),o=n(6010),a="tabItem_1uMI",l="tabItemActive_2DSg";var s=37,c=39;var p=function(e){var t=e.lazy,n=e.block,p=e.defaultValue,d=e.values,u=e.groupId,m=e.className,h=(0,i.Z)(),v=h.tabGroupChoices,f=h.setTabGroupChoices,y=(0,r.useState)(p),b=y[0],k=y[1],g=r.Children.toArray(e.children),w=[];if(null!=u){var _=v[u];null!=_&&_!==b&&d.some((function(e){return e.value===_}))&&k(_)}var N=function(e){var t=e.currentTarget,n=w.indexOf(t),r=d[n].value;k(r),null!=u&&(f(u,r),setTimeout((function(){var e,n,r,i,o,a,s,c;(e=t.getBoundingClientRect(),n=e.top,r=e.left,i=e.bottom,o=e.right,a=window,s=a.innerHeight,c=a.innerWidth,n>=0&&o<=c&&i<=s&&r>=0)||(t.scrollIntoView({block:"center",behavior:"smooth"}),t.classList.add(l),setTimeout((function(){return t.classList.remove(l)}),2e3))}),150))},C=function(e){var t,n;switch(e.keyCode){case c:var r=w.indexOf(e.target)+1;n=w[r]||w[0];break;case s:var i=w.indexOf(e.target)-1;n=w[i]||w[w.length-1]}null==(t=n)||t.focus()};return r.createElement("div",{className:"tabs-container"},r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":n},m)},d.map((function(e){var t=e.value,n=e.label;return r.createElement("li",{role:"tab",tabIndex:b===t?0:-1,"aria-selected":b===t,className:(0,o.Z)("tabs__item",a,{"tabs__item--active":b===t}),key:t,ref:function(e){return w.push(e)},onKeyDown:C,onFocus:N,onClick:N},n)}))),t?(0,r.cloneElement)(g.filter((function(e){return e.props.value===b}))[0],{className:"margin-vert--md"}):r.createElement("div",{className:"margin-vert--md"},g.map((function(e,t){return(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==b})}))))}},9443:function(e,t,n){"use strict";var r=(0,n(7294).createContext)(void 0);t.Z=r},944:function(e,t,n){"use strict";var r=n(7294),i=n(9443);t.Z=function(){var e=(0,r.useContext)(i.Z);if(null==e)throw new Error("`useUserPreferencesContext` is used outside of `Layout` Component.");return e}},9435:function(e,t,n){"use strict";n.d(t,{Z:function(){return o}});var r=n(7294),i="surveyLinkBox_YpLv";function o(e){e.docTitle;return r.createElement("a",{href:"https://github.com/pytorch/live/issues/new?assignees=&labels=Tutorial+Feedback&template=tutorial_feedback.yml",target:"_blank"},r.createElement("div",{className:i},"Share what we can improve!"))}},620:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return s},metadata:function(){return c},toc:function(){return p},default:function(){return u}});var r=n(2122),i=n(9756),o=(n(7294),n(3905)),a=(n(1395),n(8215),n(9435)),l=["components"],s={id:"prepare-custom-model",sidebar_position:4},c={unversionedId:"tutorials/prepare-custom-model",id:"tutorials/prepare-custom-model",isDocsHomePage:!1,title:"Prepare Custom Model",description:"In this tutorial, we will learn how to prepare a custom AI model so that it can be integrated into a PyTorch Live demo.",source:"@site/docs/tutorials/prepare-custom-model.mdx",sourceDirName:"tutorials",slug:"/tutorials/prepare-custom-model",permalink:"/docs/tutorials/prepare-custom-model",editUrl:"https://github.com/pytorch/live/edit/main/website/docs/tutorials/prepare-custom-model.mdx",version:"current",sidebarPosition:4,frontMatter:{id:"prepare-custom-model",sidebar_position:4},sidebar:"docs",previous:{title:"MNIST Digit Classification",permalink:"/docs/tutorials/mnist-digit-classification"},next:{title:"Server Model",permalink:"/docs/tutorials/server-model"}},p=[{value:"In this tutorial, we will learn how to prepare a custom AI model so that it can be integrated into a PyTorch Live demo.",id:"in-this-tutorial-we-will-learn-how-to-prepare-a-custom-ai-model-so-that-it-can-be-integrated-into-a-pytorch-live-demo",children:[]},{value:"Create Live Spec",id:"create-live-spec",children:[]},{value:"Bundle Live Spec with Model",id:"bundle-live-spec-with-model",children:[{value:"Set up Python virtual environment",id:"set-up-python-virtual-environment",children:[]},{value:"Install <code>torch</code> and <code>torchvision</code> dependencies",id:"install-torch-and-torchvision-dependencies",children:[]},{value:"Build the Model",id:"build-the-model",children:[]}]},{value:"Learn More About Live Spec",id:"learn-more-about-live-spec",children:[]},{value:"Give us feedback",id:"give-us-feedback",children:[]}],d={toc:p};function u(e){var t=e.components,n=(0,i.Z)(e,l);return(0,o.kt)("wrapper",(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("div",{className:"tutorial-page"},(0,o.kt)("h3",{id:"in-this-tutorial-we-will-learn-how-to-prepare-a-custom-ai-model-so-that-it-can-be-integrated-into-a-pytorch-live-demo"},"In this tutorial, we will learn how to prepare a custom AI model so that it can be integrated into a PyTorch Live demo."),(0,o.kt)("p",null,"PyTorch Live works with high-level data types such as images and strings. To run inference, these high-level data types need to be transformed into tensors and the model output needs to be transformed into high-level data types. Transforming a high-level data type into a tensor is called ",(0,o.kt)("inlineCode",{parentName:"p"},"pack")," and transforming a tensor into a high-level data type is called ",(0,o.kt)("inlineCode",{parentName:"p"},"unpack"),"."),(0,o.kt)("p",null,"Each model will have a ",(0,o.kt)("inlineCode",{parentName:"p"},"pack")," and an ",(0,o.kt)("inlineCode",{parentName:"p"},"unpack"),". Both are specified in the PyTorch Live Spec (or Live Spec for short). The Live Spec has to be bundled with the model ",(0,o.kt)("inlineCode",{parentName:"p"},"ptl")," file as ",(0,o.kt)("inlineCode",{parentName:"p"},"extra_files"),"."),(0,o.kt)("p",null,"This section will guide you step-by-step for how to bundle the Live Spec with ",(0,o.kt)("inlineCode",{parentName:"p"},"pack")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"unpack")," for an image classification model, more specifically the MobileNet V3 (small) model."),(0,o.kt)("p",null,"We can do this bundling step from anywhere in our filesystem that has access to ",(0,o.kt)("inlineCode",{parentName:"p"},"python"),". If you already have a PyTorch Live project, follow these steps from a subdirectory in the project directory, for example, ",(0,o.kt)("inlineCode",{parentName:"p"},"<PYTORCH_LIVE_PROJECT>/models"),", so that the bundled model is in place and ready to go when we're done."),(0,o.kt)("h2",{id:"create-live-spec"},"Create Live Spec"),(0,o.kt)("p",null,"Create a ",(0,o.kt)("inlineCode",{parentName:"p"},"live.spec.json")," file and insert the following definition."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json",metastring:"title=live.spec.json",title:"live.spec.json"},'{\n  "pack": {\n    "type": "tensor_from_image",\n    "image": "image",\n    "transforms": [\n      {\n        "type": "image_to_image",\n        "name": "center_crop"\n      },\n      {\n        "type": "image_to_image",\n        "name": "scale",\n        "width": 224,\n        "height": 224\n      },\n      {\n        "type": "image_to_tensor",\n        "name": "rgb_norm",\n        "mean": [0.485, 0.456, 0.406],\n        "std": [0.229, 0.224, 0.225]\n      }\n    ]\n  },\n  "unpack": {\n    "type": "argmax",\n    "dtype": "float",\n    "key": "maxIdx",\n    "valueKey": "confidence"\n  }\n}\n')),(0,o.kt)("h2",{id:"bundle-live-spec-with-model"},"Bundle Live Spec with Model"),(0,o.kt)("p",null,"Next is to download and bundle the Live Spec with the MobileNet V3 model using a Python script."),(0,o.kt)("h3",{id:"set-up-python-virtual-environment"},"Set up Python virtual environment"),(0,o.kt)("p",null,"It is recommended to run the Pyhton script in a virtual environment. Python offers a command to create a virtual environment with the following command."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"python3 -m venv venv\nsource venv/bin/activate\n")),(0,o.kt)("h3",{id:"install-torch-and-torchvision-dependencies"},"Install ",(0,o.kt)("inlineCode",{parentName:"h3"},"torch")," and ",(0,o.kt)("inlineCode",{parentName:"h3"},"torchvision")," dependencies"),(0,o.kt)("p",null,"The Python script requires ",(0,o.kt)("inlineCode",{parentName:"p"},"torch")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"torchvision"),". Use the Python package manager (",(0,o.kt)("inlineCode",{parentName:"p"},"pip3")," or simply ",(0,o.kt)("inlineCode",{parentName:"p"},"pip")," in a virtual environment) to install both dependencies."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"pip3 install torch torchvision\n")),(0,o.kt)("h3",{id:"build-the-model"},"Build the Model"),(0,o.kt)("p",null,"The following script will download the MobileNet V3 model from the PyTorch Hub, optimize it for mobile use, and bundle the Live Spec as extra file with the ",(0,o.kt)("inlineCode",{parentName:"p"},"ptl")," file."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py",metastring:"title=make_model.py",title:"make_model.py"},'from pathlib import Path\n\nimport torch\nimport torchvision\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nmodel = torchvision.models.mobilenet_v3_small(pretrained=True)\nmodel.eval()\n\nscripted_model = torch.jit.script(model)\noptimized_model = optimize_for_mobile(scripted_model)\nspec = Path("live.spec.json").read_text()\nextra_files = {}\nextra_files["model/live.spec.json"] = spec\noptimized_model._save_for_lite_interpreter("mobilenet_v3_small.ptl", _extra_files=extra_files)\n\nprint("model successfully exported")\n')),(0,o.kt)("p",null,"Create the ",(0,o.kt)("inlineCode",{parentName:"p"},"make_model.py")," file, add the Python script above, and then run Python script to create ",(0,o.kt)("inlineCode",{parentName:"p"},"mobilenet_v3_small.ptl"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"python3 make_model.py\n")),(0,o.kt)("p",null,"If you already have a PyTorch Live project, and you haven't run the commands in the ",(0,o.kt)("inlineCode",{parentName:"p"},"<PYTORCH_LIVE_PROJECT>/models")," directory, copy ",(0,o.kt)("inlineCode",{parentName:"p"},"mobilenet_v3_small.ptl")," to the ",(0,o.kt)("inlineCode",{parentName:"p"},"<PYTORCH_LIVE_PROJECT>/models")," directory. It will now be accessible to load and run in your PyTorch Live project using the path ",(0,o.kt)("inlineCode",{parentName:"p"},"models/mobilenet_v3_small.ptl"),"."),(0,o.kt)("h2",{id:"learn-more-about-live-spec"},"Learn More About Live Spec"),(0,o.kt)("p",null,"For additional details about the Live Spec JSON format and its transforms, read the ",(0,o.kt)("a",{parentName:"p",href:"/docs/api/model-spec"},"model specification")," page in the API documentation."),(0,o.kt)("h2",{id:"give-us-feedback"},"Give us feedback"),(0,o.kt)(a.Z,{docTitle:"Prepare Custom Model",mdxType:"SurveyLinkButton"})))}u.isMDXComponent=!0},6010:function(e,t,n){"use strict";function r(e){var t,n,i="";if("string"==typeof e||"number"==typeof e)i+=e;else if("object"==typeof e)if(Array.isArray(e))for(t=0;t<e.length;t++)e[t]&&(n=r(e[t]))&&(i&&(i+=" "),i+=n);else for(t in e)e[t]&&(i&&(i+=" "),i+=t);return i}function i(){for(var e,t,n=0,i="";n<arguments.length;)(e=arguments[n++])&&(t=r(e))&&(i&&(i+=" "),i+=t);return i}n.d(t,{Z:function(){return i}})}}]);